# Collect same next token contexts for various pairs/sets of language-generating models
Generate continuations with various (L)LMs and pick those that match into a dataset with higher worth for interpretability research

A project idea, that seems good for AI alignment:
1. Generate a dataset of word/token sequences, where a small and larger language models generate( with temperature=0) the same next token. I thought to start that with Lllama7B and  Lllama13B initially, but can be done with any pair actually and likely better to be started with the tiniest models like TinyStories!  I would hope it comes useful to interpretability research on LLMs.
If interested in collab, link a channel to coordinate. I am likely to kick it off after noon.

Qs: 
+ What models to try/start with?

+ Where to start the results/matches?
